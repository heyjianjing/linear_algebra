{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Least squares"
      ],
      "metadata": {
        "id": "s9JRN0lOHziu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For $y=Ax$, $A\\in \\mathbf{R}^{m \\times n}$, $m>n$, `full rank`\n",
        "\n",
        "The least squares problem finds $x_{ls}$ that `minimizes` $$\\|Ax-y\\|^2$$"
      ],
      "metadata": {
        "id": "LPIeN-tCB2ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can expand this objective as (recognizing $y^TAx$ and $x^TA^Ty$ are scalars)\n",
        "\n",
        "$$\\begin{align*}\\|Ax-y\\|^2&=(Ax-y)^T(Ax-y)\\\\\n",
        "&=(x^TA^T-y^T)(Ax-y)\\\\\n",
        "&=x^TA^TAx-2y^TAx+y^Ty\n",
        "\\end{align*}$$\n",
        "\n",
        "We set `derivative` of this w.r.t. $x$ to zero\n",
        "\n",
        "$$\\frac{\\partial}{\\partial x}\\|Ax-y\\|^2=2A^TAx-2A^Ty=0$$\n",
        "\n",
        "We get\n",
        "\n",
        "$$x_{ls}=(A^TA)^{-1}A^Ty$$\n",
        "\n",
        "which is a `left inverse` of $A$ multiplied by $y$"
      ],
      "metadata": {
        "id": "i2eTCdMzCXyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Geometric interpretation"
      ],
      "metadata": {
        "id": "n4pll0zaFehH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`By definition`, $x_{ls}$ is the point in $R(A)$ that is `closest` to $y$, that is, it is the `projection` of $y$ onto $R(A)$"
      ],
      "metadata": {
        "id": "zBLqNmsrFhJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can think that the transformation $Ax_{ls}=A(A^TA)^{-1}A^Ty$ `projects` $y$ to $R(A)$, and the projection is done through `projection matrix` associated with $R(A)$, namely, $A(A^TA)^{-1}A^T$"
      ],
      "metadata": {
        "id": "C-uvAfeEF5oO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see this, we just need to verify that the least square residual is `orthogonal` to $R(A)$\n",
        "\n",
        "$$\\begin{align*}r^T(Az)&=(Ax_{ls}-y)^T(Az)\\\\\n",
        "&=[(A(A^TA)^{-1}A^T-I)y]^T(Az)\\\\\n",
        "&=y^T(A(A^TA)^{-1}A^T-I)^T(A^T)^Tz \\\\\n",
        "& \\text{swap order under transpose} \\\\\n",
        "&=y^T\\left(A^T(A(A^TA)^{-1}A^T)-A^T\\right)^Tz \\\\\n",
        "&=y^T(A^T-A^T)^Tz\\\\\n",
        "&=0\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "wfrVy9j2GkV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LS through QR factorization"
      ],
      "metadata": {
        "id": "uUR9qZRJILVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For QR\n",
        "\n",
        "$$A=QR$$\n",
        "\n",
        "with $Q\\in \\mathbf{R}^{m \\times n}$, $Q^TQ=I_n$, $R\\in \\mathbf{R}^{n \\times n}$ and upper triangular (as $A$ is full rank)\n",
        "\n",
        "The `left inverse` becomes\n",
        "\n",
        "$$\\begin{align*}\n",
        "(A^TA)^{-1}A^T&=(R^TQ^TQR)^{-1}R^TQ^T\\\\\n",
        "&=(R^TI_nR)^{-1}R^TQ^T\\\\\n",
        "&=R^{-1}I_nR^{-T}R^TQ^T\\\\\n",
        "&=R^{-1}Q^T\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "CEHykGz6INob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `projection matrix` becomes\n",
        "\n",
        "$$AR^{-1}Q^T=\\boxed{QQ^T}$$\n",
        "\n",
        "That is, the projection matrix is directly associated with the `orthonormal basis` of vector space that a vector would be projected onto\n",
        "\n",
        "This essentially says that projection matrix is symmetric (if it is orthogonal projection)"
      ],
      "metadata": {
        "id": "F7W2IPpekzac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Intuitively`, projection matrix $QQ^T$ first resolve $y$ into $R(A)$, `discarding all components` of $y$ perpendicular to $R(A)$, before reconstructing the best approximation of $y$ in $R(A)$"
      ],
      "metadata": {
        "id": "eOsZAtSok0r3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `full QR`\n",
        "\n",
        "$$A=\\begin{bmatrix}Q_1 & Q_2\\end{bmatrix}\\begin{bmatrix}R_1 \\\\ 0\\end{bmatrix}$$\n",
        "\n",
        "with $\\begin{bmatrix}Q_1 & Q_2\\end{bmatrix}\\in \\mathbf{R}^{m\\times m}$, $R_1\\in \\mathbf{R}^{n \\times n}$\n",
        "\n",
        "`Residual` with $x_{ls}$ is\n",
        "\n",
        "$$Ax_{ls}-y=AR_1^{-1}Q_1^Ty-y=(Q_1Q_1^T-I)y$$\n",
        "\n",
        "Since $\\begin{bmatrix}Q_1 & Q_2\\end{bmatrix}$ is orthonormal basis of $\\mathbf{R}^m$\n",
        "\n",
        "$$\\begin{bmatrix}Q_1 & Q_2\\end{bmatrix}\\begin{bmatrix}Q_1 & Q_2\\end{bmatrix}^T=Q_1Q_1^T+Q_2Q_2^T=I$$\n",
        "\n",
        "We get\n",
        "\n",
        "$$Ax_{ls}-y=(Q_1Q_1^T-I)y=\\boxed{-Q_2Q_2^Ty}$$\n",
        "\n",
        "Therefore\n",
        "\n",
        "* $Q_1Q_1^T$ gives projection of $y$ onto $R(A)$\n",
        "* $I-Q_1Q_1^T=-Q_2Q_2^T$ gives projection of $y$ onto subspace `perpendicular` to $R(A)$, which is $R(A)^{\\perp}$ or $N(A^T)$. That is, this projection eliminates the components in the direction of columns of $Q_1$"
      ],
      "metadata": {
        "id": "XJo3ja7-X6Fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Some properties of projection matrix"
      ],
      "metadata": {
        "id": "KqKlfcRDgXwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If $P$ is a projection matrix\n",
        "\n",
        "* $P^T=P$\n",
        "* $P^2=P$"
      ],
      "metadata": {
        "id": "EN7qAd0clnGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$I-P$ is also a projection matrix because\n",
        "\n",
        "* $(I-P)^T=I-P^T=I-P$\n",
        "* $(I-P)^2=I-2P+P^2=I-2P+P=I-P$"
      ],
      "metadata": {
        "id": "1DYil7gVl4eh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "64VKzPhM8fWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}