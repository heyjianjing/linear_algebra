{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### `Gain` of a matrix"
      ],
      "metadata": {
        "id": "CfWMiHIGCqu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $A\\in \\mathbf{R}^{m \\times n}$ (tall, square or fat), for $x\\in \\mathbf{R}^n$\n",
        "\n",
        "$$\\frac{\\|Ax\\|}{\\|x\\|}$$\n",
        "\n",
        "gives the `amplification factor` or `gain` of $A$ `in the direction` of $x$"
      ],
      "metadata": {
        "id": "zIzK1MImE5hV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Largest` gain and `matrix norm`"
      ],
      "metadata": {
        "id": "Xxg3ty2wN2PF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The largest gain\n",
        "\n",
        "$$\\max\\frac{\\|Ax\\|}{\\|x\\|}, x\\neq0$$\n",
        "\n",
        "is known as `matrix norm` or `spectral norm` of $A$, written as $\\|A\\|$\n",
        "\n",
        "Recall the upper and lower bound of quadratic form in terms of the eigenvalues of a matrix\n",
        "\n",
        "$$\\lambda_{s,min} \\|x\\|^2 \\leq x^TSx \\leq \\lambda_{s,max} \\|x\\|^2 $$\n",
        "\n",
        "we have\n",
        "\n",
        "$$\\max\\frac{\\|Ax\\|^2}{\\|x\\|^2}=\\max\\frac{x^T(A^TA)x}{\\|x\\|^2}=\\boxed{\\lambda_{max} \\,(A^TA)}$$\n",
        "\n",
        "Therefore, the matrix norm is\n",
        "\n",
        "$$\\|A\\|=\\sqrt{\\lambda_{max} \\,(A^TA)}$$"
      ],
      "metadata": {
        "id": "Eaq5A36IN5Eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$A^TA\\in \\mathbf{R}^{n \\times n}$ is symmetric and $\\boxed{A^TA\\geq0}$"
      ],
      "metadata": {
        "id": "wEzdnlTIPO8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Smallest` gain"
      ],
      "metadata": {
        "id": "NZN8qK58OuMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly\n",
        "\n",
        "$$\\min\\frac{\\|Ax\\|}{\\|x\\|}, x\\neq0=\\sqrt{\\lambda_{min} \\,(A^TA)}$$"
      ],
      "metadata": {
        "id": "Z-m231bMOwLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So\n",
        "\n",
        "* `max gain` input direction is $x=q_1$, eigenvector of $A^TA$ associated with $\\lambda_{max}$\n",
        "* `min gain` input direction is $x=q_n$, eigenvector of $A^TA$ associated with $\\lambda_{min}$"
      ],
      "metadata": {
        "id": "ZKmhrzM0Pgwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Nullspace`, for example, represents the `directions` where gain is `zero`\n",
        "\n",
        "In case $Ax$ is tiny, which for practical purposes can be considered as $x\\in N(A)$, matrix norm gives a quantitative way of talking about `nullspace`"
      ],
      "metadata": {
        "id": "a4glyprkeHqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(formatter={'float': '{: 0.4f}'.format})"
      ],
      "metadata": {
        "id": "30G10sbbQVwG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tall_matrix=False\n",
        "\n",
        "if tall_matrix:\n",
        "    A = np.array([[1, 2],\n",
        "                  [3, 4],\n",
        "                  [5, 6]])\n",
        "else:\n",
        "    A = np.array([[1, 3, 5],\n",
        "                  [2, 4, 6]])\n",
        "\n",
        "A_TA = np.dot(A.T, A)\n",
        "\n",
        "eigenvalues, eigenvectors = np.linalg.eig(A_TA)\n",
        "\n",
        "print(f'Eigenvalues \\n{eigenvalues}')\n",
        "print(f'\\nEigenvectors \\n{eigenvectors}')\n",
        "print(f'\\nA_TA \\n{A_TA}')\n",
        "print(f'\\nReconstructed matrix \\n{eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T}')"
      ],
      "metadata": {
        "id": "KtV1_unKE5ML",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5285f303-2d12-4889-ab4c-6038ecd7ec90"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues \n",
            "[ 90.7355  0.2645  0.0000]\n",
            "\n",
            "Eigenvectors \n",
            "[[-0.2298 -0.8835  0.4082]\n",
            " [-0.5247 -0.2408 -0.8165]\n",
            " [-0.8196  0.4019  0.4082]]\n",
            "\n",
            "A_TA \n",
            "[[ 5 11 17]\n",
            " [11 25 39]\n",
            " [17 39 61]]\n",
            "\n",
            "Reconstructed matrix \n",
            "[[ 5.0000  11.0000  17.0000]\n",
            " [ 11.0000  25.0000  39.0000]\n",
            " [ 17.0000  39.0000  61.0000]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case of `fat matrix` here, smallest gain is zero as $A^TA$ is low rank and is guaranteed to have non-trivial `nullspace`"
      ],
      "metadata": {
        "id": "8W-sy7Lsejd7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pjvaZ_yDQlKw"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}