{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPa//bjbo9OHEIaH9tAQeuS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### Arnoldi iteration"],"metadata":{"id":"ZN-fQUPvnIVb"}},{"cell_type":"markdown","source":["Previously we see that Arnoldi iteration can produce\n","\n","$$AQ_n=Q_{n+1}H_{n+1, n}$$\n","\n","and\n","\n","$$H_n = Q_n^TAQ_n$$\n","\n","with $H_n$ in upper Hessenberg form\n","\n","In addition, we have\n","\n","$$\\text{span}(q_1, q_2, \\cdots, q_n)=K_n(A, b)=\\text{span}(b, Ab, \\cdots, A^{n-1}b)$$\n","\n","that is, the process iteratively generates `orthonormal basis` for Krylov subspace $K_n(A, b)$"],"metadata":{"id":"TW8AZI3onKyT"}},{"cell_type":"markdown","source":["#### Solving linear system of equations"],"metadata":{"id":"43iikgzF6tuN"}},{"cell_type":"markdown","source":["One application of the Arnoldi iteration, other than helping to compute eigenvalues, is to solve linear system of equations $Ax=b$, where $A\\in \\mathbf{R}^{m \\times m}$ is `full rank`\n","\n","The idea is that at each iteration, we seek an approximate solution within the `Krylov subspace`"],"metadata":{"id":"BNIJhc_h6vxy"}},{"cell_type":"markdown","source":["For example, at `nth` iteration, we let $x^n\\in K_n(A,b)$, where $K_n(A, b)=\\text{span}(b, Ab, \\cdots, A^{n-1}b)$\n","\n","We can put $b, Ab, \\cdots, A^{n-1}b$ into a matrix $K_n$\n","\n","$$K_n=\\begin{bmatrix}b& Ab& \\cdots& A^{n-1}b\\end{bmatrix}$$\n","\n","and the idea of `generalized minimal residuals` (GMRES) method is to solve the least squares problem for coefficients $c$ of the expansion of $x^n$ in the columns of $K_n$\n","\n","$$\\min_c \\|AK_nc-b\\|$$\n","\n","Once we get $c$, we compute $x^n=K_nc$\n","\n","While this representation is correct, in practice, it's more efficient to work with an orthonormal basis"],"metadata":{"id":"KEzGrYxItYGj"}},{"cell_type":"markdown","source":["With Arnoldi iteration, we know that $Q_n$ is the orthonormal basis for $K_n$ and since $AQ_n=Q_{n+1}H_{n+1,n}$, we can rewrite the optimization as\n","\n","$$\\begin{align*}\n","&\\min_c \\|AK_nc-b\\|\\\\\n","& \\text{use y as coefficients for expansion of x in } Q_n  \\\\\n","=&\\min_y \\|AQ_ny-b\\| \\\\\n","=&\\min_y \\|Q_{n+1}H_{n+1,n}y-b\\| \\\\\n","& \\text{multiplying orthogonal matrix does not change norm}  \\\\\n","=&\\min_y \\|H_{n+1,n}y-Q_{n+1}^Tb\\| \\\\\n","& \\text{Arnoldi uses normalized b as first q by design}  \\\\\n","=&\\min_y \\|H_{n+1,n}y-\\|b\\|e_1\\| \\\\\n","\\end{align*}$$\n","\n","Once we get $y$, we compute $x^n$ as\n","\n","$$x^n=Q_ny$$\n","\n","We repeat until the residual $\\|H_{n+1,n}y-\\|b\\|e_1\\|$ is below certain threshold"],"metadata":{"id":"nkuK3KrQuyYY"}},{"cell_type":"markdown","source":["#### Convergence of GMRES"],"metadata":{"id":"NajcCOBy8R6e"}},{"cell_type":"markdown","source":["It can be shown that what GMRES does is to find $p\\in P_n$, where $P_n$ is a set of all polynomials of degree $\\leq n$ with $p(0)=1$, such that\n","\n","$$\\|p(A)b\\|$$\n","\n","is minimized"],"metadata":{"id":"OAUbp3ca_PPV"}},{"cell_type":"markdown","source":["`Convergence theorem` from NLA book\n","\n","Suppose $A$ is diagonalizable $A=V\\Lambda V^{-1}$, at step $n$ of GMRES, the residual $r_n$ satisfies\n","\n","$$\\frac{\\|r_n\\|}{\\|b\\|}\\leq \\kappa(V) \\inf_{p\\in P_n}\\|p\\|_{\\Lambda(A)}$$\n","\n","where $\\Lambda(A)$ is set of eigenvalues of $A$"],"metadata":{"id":"vs3qlS2tD1Ns"}},{"cell_type":"markdown","source":["$\\|p\\|_{\\Lambda(A)}$ is defined as\n","\n","$$\\|p\\|_{\\Lambda(A)}=\\sup_{z\\in \\Lambda(A)} |p(z)|$$\n","\n","This expression means we're taking the supremum of the absolute value of the polynomial $p(z)$ evaluated at each eigenvalue $z$ of $A$. Essentially, it's the largest value that $|p(z)|$ attains over all eigenvalues of $A$"],"metadata":{"id":"Rql43Vwk9HxI"}},{"cell_type":"markdown","source":["##### Interpretation"],"metadata":{"id":"cUUcQONgBYO2"}},{"cell_type":"markdown","source":["* We want a polynomial $p$, such that when we evaluate it at `any eigenvalue` of $A$, the absolute value $|p(z)|$ is `as small as possible`, that is, the polynomial can `uniformly` approximate zero over all eigevalues\n","* Further, if $V$ is well-conditioned, the bound is tighter, leading to potentially faster convergence"],"metadata":{"id":"778cTofA__I_"}},{"cell_type":"markdown","source":["##### Judging matrix suitability for GMRES"],"metadata":{"id":"2YIAgomrBWPQ"}},{"cell_type":"markdown","source":["* We prefer `clustered` eigenvalues, which is easier to find a polynomial that is small over all of them\n","* Eigenvalues `close to zero`, if eigenvalues are widely spread out, higher-degree polynomials may be needed, slowing down convergence\n","* Smaller $\\kappa(V)$ implies $A$ is closer to `diagonalizable with an orthogonal matrix`, since orthogonal matrix has the smallest condition number of one\n","\n","For example, symmetric matrices with orthogonal $V$ and eigenvalues that are relatively close and small, GMRES should converge well"],"metadata":{"id":"bpx2rbHSBfgk"}},{"cell_type":"markdown","source":["##### Challenges"],"metadata":{"id":"VdD1wIk2NCG6"}},{"cell_type":"markdown","source":["For general matrices, it is difficult to estimate $\\kappa(V)$\n","\n","In addition, while power iteration can be used to estimate the largest eigenvalue of $A$, finding the smallest can be challenging for large matrices, a few Arnoldi/Lanczos iterations may provide some idea regarding the distribution of eigenvalues"],"metadata":{"id":"C58eSDwqNDR7"}},{"cell_type":"markdown","source":["#### Example"],"metadata":{"id":"ncFPaeqU6GH6"}},{"cell_type":"markdown","source":["From NLA book, there is an example of ideal case\n","\n","* Eigenvalues clustered near one by using identity matrix plus some small perturbation\n","* Well-conditioned $V$ since the matrix is relatively close to identity matrix"],"metadata":{"id":"A48K94a0CTsH"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","\n","np.set_printoptions(formatter={'float': '{: 0.4f}'.format})\n","\n","plt.style.use('dark_background')\n","# color: https://matplotlib.org/stable/gallery/color/named_colors.htm"],"metadata":{"id":"E7IXLxroydqP","executionInfo":{"status":"ok","timestamp":1732989308853,"user_tz":300,"elapsed":241,"user":{"displayName":"Jianjing Zhang","userId":"07583447684872889447"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def general_gram_schmidt(A, modified=True):\n","    _, k = A.shape  # Get number of vectors (columns) in A\n","    Q = []  # Start with empty list, as we don't know how many q's are there\n","    R = np.zeros((0, k))  # Same here\n","\n","    for i in range(k):\n","        # Loop over all a_i\n","        q = A[:, i].copy()\n","\n","        # This skips when i=0\n","        for j in range(len(Q)):\n","            if modified:\n","                R[j, i] = np.dot(Q[j], q)\n","            else:\n","                R[j, i] = np.dot(Q[j], A[:, i])\n","            q -= R[j, i] * Q[j]\n","\n","        # Compute norm of new q\n","        norm_q = np.sqrt(np.dot(q, q))\n","\n","        # Only add q to Q if it is not small\n","        if norm_q > 1e-10:  # Tolerance\n","            q /= norm_q\n","            Q.append(q)\n","\n","            # Expand R to include new row corresponding to new q\n","            new_row = np.zeros((1, k))\n","            new_row[0, i] = norm_q\n","            R = np.vstack([R, new_row])\n","\n","    Q = np.column_stack(Q)  # Convert to array\n","\n","    return Q, R\n","\n","def back_substitution(R, b):\n","    m, n = R.shape\n","    x = np.zeros(n)\n","    for i in range(n - 1, -1, -1):\n","        x[i] = (b[i] - np.dot(R[i, i + 1:], x[i + 1:])) / R[i, i]\n","    return x"],"metadata":{"id":"ChKgyw3S9iav","executionInfo":{"status":"ok","timestamp":1732989309104,"user_tz":300,"elapsed":3,"user":{"displayName":"Jianjing Zhang","userId":"07583447684872889447"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def gmres(A, b, tol=1e-10, n_restart=50, n_max=20):\n","    m, n_ = A.shape\n","    x = np.zeros(n_)\n","    r = b - A @ x\n","    residuals = [np.linalg.norm(r)]\n","    iter_count = 0\n","    n = min(n_, n_max)\n","\n","    for k in range(n_restart):\n","        r_norm = np.linalg.norm(r)\n","        Q = np.zeros((m, n+1))\n","        H = np.zeros((n+1, n))\n","        Q[:, 0] = r / r_norm\n","\n","        for i in range(n):\n","            iter_count += 1\n","            # Arnoldi iteration\n","            v = A @ Q[:, i]\n","\n","            for j in range(i+1):\n","                H[j, i] = Q[:, j] @ v\n","                v -= H[j, i] * Q[:, j]\n","\n","            H[i+1, i] = np.linalg.norm(v)\n","            if H[i+1, i] < tol:  # Early termination\n","                H = H[:i+2, :i+1]  # Truncate H\n","                Q = Q[:, :i+1]     # Truncate Q\n","                break\n","            Q[:, i+1] = v / H[i+1, i]\n","\n","            # Least squares\n","            e1 = np.zeros(i+2) # Size equalling first dim of H_{n+1,n}, note i+2 since i starts at 0\n","            e1[0] = r_norm\n","\n","            # H_{n+1,n} y = r_norm * e1\n","            # y, _, _, _ = np.linalg.lstsq(H[:i+2, :i+1], e1, rcond=None) # NumPy solver\n","            Q_ls, R_ls = general_gram_schmidt(H[:i+2, :i+1])\n","            y = back_substitution(R_ls, Q_ls.T @ e1)\n","\n","            # Previous residual drop taken care of by x\n","            x_n = x + Q[:, :i+1] @ y # Current residual drop taken care of by Q[:, :i+1] @ y\n","\n","            residual_norm = np.linalg.norm(b - A @ x_n)\n","            residuals.append(residual_norm)\n","\n","            if residual_norm < tol:\n","                print(f\"Converged at iteration {iter_count}\")\n","                return x_n, residuals\n","\n","        print(f\"Iteration {iter_count}, residual norm: {residual_norm}\")\n","\n","        x = x_n\n","        r = b - A @ x # Fit only remaining residual for next restart\n","\n","    print(\"Maximum iterations reached without convergence\")\n","    return x, residuals"],"metadata":{"id":"b8sYJfCy0LLz","executionInfo":{"status":"ok","timestamp":1732989309105,"user_tz":300,"elapsed":3,"user":{"displayName":"Jianjing Zhang","userId":"07583447684872889447"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["np.random.seed(42)\n","\n","m = 3000\n","A = 1 * np.eye(m) + 0.5 * np.random.rand(m, m)/np.sqrt(m) # Ideal case\n","# A = np.random.rand(m, m) # Non-ideal case\n","print(\"Condition number:\", np.linalg.cond(A))\n","x_true = np.random.rand(m)\n","b = A @ x_true\n","n_restart = 100\n","n_max = 1000\n","\n","start_time = time.time()\n","x_approx, residuals = gmres(A, b, tol=1e-10, n_restart=n_restart, n_max = n_max)\n","print(f\"Time taken: {time.time() - start_time} seconds\")\n","\n","print(\"x difference:\", np.linalg.norm(x_true - x_approx))\n","print(\"b difference:\", np.linalg.norm(b - A @ x_approx))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XsGFeHzR_J_y","executionInfo":{"status":"ok","timestamp":1732989324542,"user_tz":300,"elapsed":15440,"user":{"displayName":"Jianjing Zhang","userId":"07583447684872889447"}},"outputId":"a9e06435-517f-4e72-d843-dc67a419b3de"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Condition number: 18.3223509474835\n","Converged at iteration 15\n","Time taken: 0.22419190406799316 seconds\n","x difference: 2.4002373922867235e-11\n","b difference: 2.3165023797534836e-11\n"]}]},{"cell_type":"markdown","source":["#### Comparison with LU with partial pivoting"],"metadata":{"id":"1p4lpalqIFuA"}},{"cell_type":"code","source":["def lu_factorization(A):\n","    m, n = A.shape\n","    u_mat = A.copy().astype(float)\n","    l_mat = np.identity(m)\n","    p_mat = np.identity(m)\n","\n","    for k in range(m-1):\n","        # Find pivot\n","        pivot = np.argmax(np.abs(u_mat[k:, k])) + k\n","\n","        if pivot != k:\n","            # Swap rows in u, p, and l\n","            u_mat[[k, pivot], :] = u_mat[[pivot, k], :]\n","            p_mat[[k, pivot], :] = p_mat[[pivot, k], :]\n","            l_mat[[k, pivot], :k] = l_mat[[pivot, k], :k]\n","\n","        for j in range(k + 1, m):\n","            l_mat[j, k] = u_mat[j, k] / u_mat[k, k]\n","            # Subtract multiply of kth row from jth row\n","            u_mat[j, k:] -= l_mat[j, k] * u_mat[k, k:]\n","\n","    return p_mat, l_mat, u_mat\n","\n","def forward_substitution(L, b):\n","    m, n = L.shape\n","    x = np.zeros(n)\n","    for i in range(n):\n","        x[i] = (b[i] - np.dot(L[i, :i], x[:i])) / L[i, i]\n","    return x"],"metadata":{"id":"mG5NVDwO4UFE","executionInfo":{"status":"ok","timestamp":1732989324542,"user_tz":300,"elapsed":3,"user":{"displayName":"Jianjing Zhang","userId":"07583447684872889447"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["start_time = time.time()\n","p, l, u = lu_factorization(A)\n","y_lu = forward_substitution(l, p @ b)\n","x_lu = back_substitution(u, y_lu)\n","print(f\"Time taken: {time.time() - start_time} seconds\")\n","\n","print(\"x difference:\", np.linalg.norm(x_lu - x_true))\n","print(\"b difference:\", np.linalg.norm(b - A @ x_lu))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uLz47SqAIOHV","executionInfo":{"status":"ok","timestamp":1732989359988,"user_tz":300,"elapsed":35448,"user":{"displayName":"Jianjing Zhang","userId":"07583447684872889447"}},"outputId":"0bf60cd0-907f-44fd-b05a-356234dac32a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Time taken: 35.39543128013611 seconds\n","x difference: 1.310488382211915e-13\n","b difference: 1.3151040447837372e-13\n"]}]}]}