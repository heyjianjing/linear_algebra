{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Low-rank approximation overview"
      ],
      "metadata": {
        "id": "2PSiR8yv_Xgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have a matrix $A\\in \\mathbf{R}^{m\\times n}$, but we don't want to store all $mn$ entries, we can try to decompose $A$ as\n",
        "\n",
        "$$A=BX$$\n",
        "\n",
        "where $B\\in \\mathbf{R}^{m\\times k}$ and $X\\in \\mathbf{R}^{k\\times n}, k\\ll m, n$, which means we only need to store $k(m+n)$ entries\n",
        "\n",
        "This is known as low-rank approximation since $B$ and $X$ have rank of $k$ at most"
      ],
      "metadata": {
        "id": "YQS3bynt_kp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say we randomly choose $k$ columns of $A$ to construct $B$\n",
        "\n",
        "To obtain $X$, we basically solve the least squares problem\n",
        "\n",
        "$$\\min \\|A-BX\\|^2$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\\min \\|a_i-Bx_i\\|^2, i=1, \\cdots, n$$\n",
        "\n",
        "With normal equation, we have\n",
        "\n",
        "$$x_i=(B^TB)^{-1}B^Ta_i$$\n",
        "\n",
        "and\n",
        "\n",
        "$$X=(B^TB)^{-1}B^TA$$\n",
        "\n",
        "Therefore, our low-rank approximation is\n",
        "\n",
        "$$A\\approx B(B^TB)^{-1}B^TA$$\n",
        "\n",
        "where $(B^TB)^{-1}B^TA\\in \\mathbf{R}^{k\\times n}$\n",
        "\n",
        "But, what is the `best` low-rank approximation of $A$?"
      ],
      "metadata": {
        "id": "FXaccZhaAmlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Low-rank approximation with SVD"
      ],
      "metadata": {
        "id": "XnGph7Pbh8zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $A\\in \\mathbf{R}^{m \\times n}$ (tall, square or fat), its SVD is given by\n",
        "\n",
        "$$A=U\\Sigma V^T=\\sum_{i=1}^r\\sigma_i u_i v_i^T$$\n",
        "\n",
        "where\n",
        "* $A\\in \\mathbf{R}^{m \\times n}$, $\\text{rank}(A)=r$\n",
        "* $U\\in \\mathbf{R}^{m \\times r}$, $U^TU=I$\n",
        "* $V\\in \\mathbf{R}^{n \\times r}$, $V^TV=I$\n",
        "* $\\Sigma =\\text{diag}(\\sigma_1, \\cdots, \\sigma_r)$, $\\sigma_1 \\geq\\cdots\\geq \\sigma_r > 0$"
      ],
      "metadata": {
        "id": "zIzK1MImE5hV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want a matrix $\\bar{A}$ with $\\text{rank}(\\bar{A})\\leq p < r$, then the following gives the `optimal approximation` with `minimized` $\\|A-\\bar{A}\\|$\n",
        "\n",
        "$$\\bar{A}=\\sum_{i=1}^p\\sigma_i u_i v_i^T$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\|A-\\bar{A}\\|=\\left\\|\\sum_{i=p+1}^r\\sigma_i u_i v_i^T\\right\\|=\\boxed{\\sigma_{p+1}}$$"
      ],
      "metadata": {
        "id": "WgmJMEWykRYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show this, assume $B$ is another low-rank approximation with $\\text{rank}(B)\\leq p$\n",
        "\n",
        "Then, the dimension of `nullspace` $\\dim N(B) \\boxed{\\geq n-p}$, as nullspace and $R(B^T)$ are complementary in $\\mathbf{R}^n$ and $R(B^T)\\leq p$\n",
        "\n",
        "Also, we know that $\\dim \\text{span}(v_1, \\cdots, v_{p+1})\\boxed{=p+1}$\n",
        "\n",
        "Therefore, $N(B)$ and $\\text{span}(v_1, \\cdots, v_{p+1})$ must `intersect`, that is, there is a unit vector $z\\in \\mathbf{R}^n$ such that\n",
        "\n",
        "$$Bz=0\\, \\text{and} \\, z\\in \\text{span}(v_1, \\cdots, v_{p+1})$$"
      ],
      "metadata": {
        "id": "L-GQwShyp6lJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can write\n",
        "\n",
        "$$(A-B)z=Az=\\sum_{i=1}^{p+1}\\sigma_i u_i v_i^Tz$$\n",
        "\n",
        "There are only $p+1$ terms as $z$ is orthogonal to $v_{p+2},\\cdots$\n",
        "\n",
        "Take Euclidean norm on each side and for right hand side using $u_i$ as basis\n",
        "\n",
        "$$\\|(A-B)z\\|^2=\\sum_{i=1}^{p+1}\\sigma_i^2 (v_i^Tz)^2\\geq \\sigma_{p+1}^2\\sum_{i=1}^{p+1} (v_i^Tz)^2=\\sigma_{p+1}^2\\|z\\|^2$$\n",
        "\n",
        "The last equality holds since $z\\in \\text{span}(v_1, \\cdots, v_{p+1})$\n",
        "\n",
        "Move $\\|z\\|^2$ to the left and use definition of `matrix norm`\n",
        "\n",
        "$$\\|A-B\\|\\geq \\sigma_{p+1} = \\|A-\\bar{A}\\|$$\n",
        "\n",
        "This means SVD gives the low-rank approximation with `smallest` $\\|A-\\bar{A}\\|$"
      ],
      "metadata": {
        "id": "KPACDliIsiKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essentially, SVD ranks all $u_iv_i^T$ in order of `importance` as determined by $\\sigma_i$, then we take as many as we could afford or need, in this case, first $p$ of them"
      ],
      "metadata": {
        "id": "jZ5MFul5lc4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Singular value as distance to singularity"
      ],
      "metadata": {
        "id": "RarJ1hDmtet5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With low-rank approximation, we can interpret singular value $\\sigma_i$ as\n",
        "\n",
        "$$\\sigma_i=\\min \\{\\|A-B\\| \\, | \\, \\text{rank}(B)\\leq i-1\\}$$\n",
        "\n",
        "That is, the `distance` (measured by matrix norm) to the nearest rank $i-1$ matrix\n",
        "\n",
        "For example\n",
        "\n",
        "* if $\\sigma_3$ is small, then, it means the matrix $A$ is very close to a `rank 2` matrix\n",
        "* for symmetric matrix, if $\\sigma_n=\\sigma_{\\min}$ is small, then, it means the matrix is very close to rank $n-1$ matrix (which would be `singular`)"
      ],
      "metadata": {
        "id": "cGZZ5pRethw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model simplification"
      ],
      "metadata": {
        "id": "pisdfv8TwZwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For $y=Ax+v$, if we know\n",
        "\n",
        "* $A\\in \\mathbf{R}^{100 \\times 30}$ has singular values: $10, 7, 2, 0.5, 0.01,\\cdots, 0.0001$\n",
        "* $\\|x\\|$ is on the order of 1\n",
        "* noise has norm on the order of 0.1\n",
        "\n",
        "Then, the terms $\\sigma_i u_i v_i^T x$ for $i\\geq 5$ are `substantially smaller than noise term`\n",
        "\n",
        "So we can simplify the model to\n",
        "\n",
        "$$y=\\sum_{i=1}^4\\sigma_i u_i v_i^T x+v$$"
      ],
      "metadata": {
        "id": "G8CjxG9wwcX4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R50YjyTxhKYS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}