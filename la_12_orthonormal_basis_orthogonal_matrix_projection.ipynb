{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Orthonormal basis"
      ],
      "metadata": {
        "id": "RzSDlfgPUCzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If columns of $U$ form an orthonormal basis for $\\mathbf{R}^n$\n",
        "\n",
        "$$U=\\begin{bmatrix}u_1 & u_2 &\\cdots&  u_n\\end{bmatrix}$$\n",
        "\n",
        "then\n",
        "\n",
        "$$u_i^T u_j = \\left\\{\\begin{array}{cl}0 &i\\neq j \\\\1 &i=j \\end{array}\\right.$$"
      ],
      "metadata": {
        "id": "uqk-2uJYcHN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### General properties"
      ],
      "metadata": {
        "id": "0Bg_MzKJdc8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For orthogonal matrix, we have\n",
        "\n",
        "$$U^TU=I$$\n",
        "\n",
        "We see $U$ has a `left inverse` $U^T$\n",
        "\n",
        "From previously, we know for invertible square matrix $U$, the `right inverse` is the same as `left inverse`\n",
        "\n",
        "Therefore\n",
        "\n",
        "$$UU^T=I$$\n",
        "\n",
        "Since for $U$\n",
        "\n",
        "$$U^{-1}U=I$$\n",
        "\n",
        "we have\n",
        "\n",
        "$$\\boxed{U^{-1}=U^T}$$"
      ],
      "metadata": {
        "id": "Qlwr3-5ldgLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From $UU^T=I$, we see that each $x\\in \\mathbf{R}^n$ can be expressed as follows\n",
        "\n",
        "$$x=\\sum_{i=1}^n(u_i^Tx)u_i$$\n",
        "\n",
        "* $u_i^Tx$: component (coefficient) of $x$ in direction of $u_i$\n",
        "* $a=U^Tx$: `resolve` $x$ into coefficients of expansion of $x$ in $u_i$'s\n",
        "* $x=Ua$: `reconstruct` $x$ from $u_i$'s using these coefficients"
      ],
      "metadata": {
        "id": "9w0It5vpiuJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Orthonormal basis simplifies expression"
      ],
      "metadata": {
        "id": "N5VwQ7RQlNh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the benefit of orthonormal basis, we can instead take an `arbitrary` basis for $\\mathbf{R}^n$\n",
        "\n",
        "$$\\begin{bmatrix}a_1 & a_2 &\\cdots&  a_n\\end{bmatrix}$$\n",
        "\n",
        "and see how we represent each $x\\in \\mathbf{R}^n$ using these basis vectors\n",
        "\n",
        "$$x=\\sum_{j=1}^n \\beta_j a_j$$\n",
        "\n",
        "We can compute the coefficient $\\beta_i$ as follows, first notice that for any $a_i, i=1, \\cdots, n$\n",
        "\n",
        "$$a_i^Tx=a_i^T\\sum_{j=1}^n \\beta_j a_j= \\sum_{j=1}^n \\beta_ja_i^Ta_j$$\n",
        "\n",
        "and we can write in matrix form\n",
        "\n",
        "$$\\begin{bmatrix}a_1^Tx \\\\a_2^Tx\\\\ \\vdots \\\\a_n^Tx\\end{bmatrix}=\\begin{bmatrix}a_1^Ta_1 & a_1^Ta_2 & \\cdots & a_1^Ta_n \\\\ a_2^Ta_1 & a_2^Ta_2 & \\cdots & a_2^Ta_n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_n^Ta_1 & a_n^Ta_2 & \\cdots & a_n^Ta_n\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\\\beta_2\\\\ \\vdots \\\\\\beta_n\\end{bmatrix}$$\n",
        "\n",
        "To get $\\beta_i$, we need to solve this linear system of equations\n",
        "\n",
        "Since $a_i$ belongs to an arbitrary basis, we can only assume that the matrix here is dense, and each $\\beta_i$ is `influenced by all basis vectors`\n",
        "\n",
        "However, if the basis is `orthonormal`, the `off-diagonal` terms in the matrix become zero, and each $\\beta_i$ can be `independently` determined using $u_i^Tx$, leads to the expression we see earlier\n",
        "\n",
        "$$x=\\sum_{i=1}^n(u_i^Tx)u_i$$"
      ],
      "metadata": {
        "id": "mkWMBC2QlV39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Geometric properties"
      ],
      "metadata": {
        "id": "Eq1xgc4-ptrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* multiplication by $U$ does not change `norm`\n",
        "\n",
        "$$\\|w\\|^2=\\|Uz\\|^2=(Uz)^T(Uz)=z^TU^TUz=z^Tz=\\|z\\|^2$$"
      ],
      "metadata": {
        "id": "EHgSANJDpxhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `inner product` is preserved\n",
        "\n",
        "$$(Uz)^T(Uz')=z^TU^TUz'=z^Tz'$$"
      ],
      "metadata": {
        "id": "DFs995xLqPom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As norms and inner products are preserved, `angles` are preserved due to geometric interpretation of inner product"
      ],
      "metadata": {
        "id": "jYPSfuZfqkYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Orthogonal matrix"
      ],
      "metadata": {
        "id": "7B8JqO03V8FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Real `square` matrix whose columns (and rows) are `orthonormal` set"
      ],
      "metadata": {
        "id": "JhrvxsKsV-UI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) if $U$ and $V$ are orthogonal, then so is $UV$\n",
        "\n",
        "$$(UV)^T(UV)=V^TU^TUV=V^TV=I$$"
      ],
      "metadata": {
        "id": "ZI_Im1ozWEq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) if $U$ is orthogonal, then so is $U^{-1}$\n",
        "\n",
        "$$U^{-1}=U^T$$\n",
        "\n",
        "Take inverse of both sides\n",
        "\n",
        "$$U=(U^T)^{-1}=(U^{-1})^T$$\n",
        "\n",
        "Plug into $UU^T=I$, we get\n",
        "\n",
        "$$(U^{-1})^TU^{-1}=I$$\n",
        "\n",
        "and we see $U^{-1}$ is orthogonal"
      ],
      "metadata": {
        "id": "Vkkg_NoqcvP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Projection onto a subspace"
      ],
      "metadata": {
        "id": "KIsQtPxKcWow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can project a vector $v\\in \\mathbf{R}^n$ onto a subspace spanned by orthonormal basis $u_i,\\cdots, u_k$ following\n",
        "\n",
        "$$\\text{proj}_{u_i,\\cdots, u_k}(v) = \\sum_{i=1}^k (u_i^Tv)u_i=\\sum_{i=1}^k (u_iu_i^T)v$$\n",
        "\n",
        "These expressions are equal (since $u_i^Tv$ is scalar), but have different interpretations\n",
        "\n",
        "* In the first case, we view the projected vector as a sum of `coefficients` $u_i^Tv$ times vectors $u_i$\n",
        "\n",
        "* In the second case, we view the projected vector as a sum of `orthogonal projections` of $v$ onto the various directions, where the `ith` projection operation is achieved by the corresponding `rank-one projection matrix` $u_iu_i^T$\n",
        "\n",
        "More on projection when discussing least squares..."
      ],
      "metadata": {
        "id": "BAq7ryWDcanM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nqMqYQOmIHLt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}