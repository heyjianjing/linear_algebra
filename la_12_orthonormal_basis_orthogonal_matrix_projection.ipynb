{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### `Orthonormal` basis"
      ],
      "metadata": {
        "id": "RzSDlfgPUCzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, we mentioned that the `simpliest basis` for $\\mathbf{R}^n$ is $e_1, e_2, \\cdots, e_n$, which are `orthonormal`, that is\n",
        "\n",
        "$$e_i^Te_j = \\left\\{\\begin{array}{cl}0 &i\\neq j \\\\1 &i=j \\end{array}\\right.$$"
      ],
      "metadata": {
        "id": "r6vwID_RV_Vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More generally, if `columns` of $U$ form an orthonormal basis for $\\mathbf{R}^n$\n",
        "\n",
        "$$U=\\begin{bmatrix}u_1 \\, u_2 \\cdots\\,  u_n\\end{bmatrix}$$\n",
        "\n",
        "Then\n",
        "\n",
        "$$u_i^T u_j = \\left\\{\\begin{array}{cl}0 &i\\neq j \\\\1 &i=j \\end{array}\\right.$$"
      ],
      "metadata": {
        "id": "uqk-2uJYcHN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### General `properties`"
      ],
      "metadata": {
        "id": "0Bg_MzKJdc8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For orthogonal matrix, we have\n",
        "\n",
        "$$U^TU=I$$\n",
        "\n",
        "We see $U$ has a `left inverse` $U^T$. From previously, we know for square matrix $U$, the `right inverse` is the same as `left inverse`. Therefore\n",
        "\n",
        "$$UU^T=I$$\n",
        "\n",
        "From this we know that `rows` of $U$ are also orthonormal\n",
        "\n",
        "Since for $U$\n",
        "\n",
        "$$U^{-1}U=I$$\n",
        "\n",
        "we have\n",
        "\n",
        "$$\\boxed{U^{-1}=U^T}$$"
      ],
      "metadata": {
        "id": "Qlwr3-5ldgLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From $UU^T=I$, we see that each $x\\in \\mathbf{R}^n$ can be expressed as follows\n",
        "\n",
        "$$x=\\sum_{i=1}^n(u_i^Tx)u_i$$\n",
        "\n",
        "* $u_i^Tx$: component (coefficient) of $x$ in direction of $u_i$\n",
        "* $a=U^Tx$: `resolve` $x$ into coefficients of expansion of $x$ in $u_i$'s\n",
        "* $x=Ua$: `reconstruct` $x$ from $u_i$'s using these coefficients"
      ],
      "metadata": {
        "id": "9w0It5vpiuJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Geometric` properties"
      ],
      "metadata": {
        "id": "Eq1xgc4-ptrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* multiplication by $U$ does not change `norm`\n",
        "\n",
        "$$\\|w\\|^2=\\|Uz\\|^2=(Uz)^T(Uz)=z^TU^TUz=z^Tz=\\|z\\|^2$$"
      ],
      "metadata": {
        "id": "EHgSANJDpxhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `inner product` is preserved\n",
        "\n",
        "$$(Uz)^T(Uz')=z^TU^TUz'=z^Tz'$$"
      ],
      "metadata": {
        "id": "DFs995xLqPom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As norms and inner products are preserved, `angles` are preserved due to geometric interpretation of inner product"
      ],
      "metadata": {
        "id": "jYPSfuZfqkYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Orthogonal` matrix"
      ],
      "metadata": {
        "id": "7B8JqO03V8FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Real `square` matrix whose columns (and rows) are `orthonormal` set"
      ],
      "metadata": {
        "id": "JhrvxsKsV-UI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) if $U$ and $V$ are orthogonal, then so is $UV$\n",
        "\n",
        "$$(UV)^T(UV)=V^TU^TUV=V^TV=I$$"
      ],
      "metadata": {
        "id": "ZI_Im1ozWEq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) if $U$ is orthogonal, then so is $U^{-1}$\n",
        "\n",
        "$$U^{-1}=U^T$$\n",
        "\n",
        "Take inverse of both sides\n",
        "\n",
        "$$U=(U^T)^{-1}=(U^{-1})^T$$\n",
        "\n",
        "Plug into $UU^T=I$, we get\n",
        "\n",
        "$$(U^{-1})^TU^{-1}=I$$\n",
        "\n",
        "and we see $U^{-1}$ is orthogonal"
      ],
      "metadata": {
        "id": "Vkkg_NoqcvP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Projection` onto a subspace"
      ],
      "metadata": {
        "id": "KIsQtPxKcWow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can project a vector $v\\in \\mathbf{R}^n$ onto a subspace spanned by orthonormal basis $u_i,\\cdots, u_k$ following\n",
        "\n",
        "$$\\text{proj}_{u_i,\\cdots, u_k}(v) = \\sum_{i=1}^k (u_i^Tv)u_i=\\sum_{i=1}^k (u_iu_i^T)v$$\n",
        "\n",
        "These expressions are equal, but have different interpretations\n",
        "\n",
        "* In the first case, we view the projected vector as a sum of `coefficients` $u_i^Tv$ times vectors $u_i$\n",
        "\n",
        "* In the second case, we view the projected vector as a sum of `orthogonal projections` of $v$ onto the various directions $q_i$, where the `ith` projection operation is achieved by the corresponding `rank-one projection matrix` $u_iu_i^T$\n",
        "\n",
        "More on projection when discussing least squares..."
      ],
      "metadata": {
        "id": "BAq7ryWDcanM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nqMqYQOmIHLt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}